{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fyMqv0I6N7V"
      },
      "source": [
        "Adapted from [Keras GitHub Example](http://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) and [Yassine Ghouzam's NB](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)\n",
        "\n",
        "TensorFlow 1.x --> TensorFlow 2.x\n",
        "\n",
        "SciKit Learn --> TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqubie9M6N7a"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP-TbLBC6N7a"
      },
      "source": [
        "This tutorial is an introduction to Convolutional Neural Networks using TensorFlow 2.x Keras API. The dataset that we will work it is the MNIST dataset, a dataset of handwritten digits 0-9, and we will use a Sequential CNN to predict which digit was drawn.\n",
        "\n",
        "This model reaches 99.3% accuracy.\n",
        "\n",
        "To prepare our notebook, run the next cell to import the necessary packages and change the accelerator from ```None``` to ```GPU```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhS7EGDb6N7c",
        "outputId": "1b9e418e-c569-4c02-a2fe-87e2b48b6f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras import backend as K\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhJiq6jg6N7e"
      },
      "source": [
        "# 2. Data Preprocessing\n",
        "\n",
        "Before building any ML model, it is important to preprocess the data. In fact, data preprocessing will generally take up the most time in any ML pipeline. The following module goes over the steps to preprocess the MNIST dataset for our purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14FxXkn36N7f"
      },
      "source": [
        "## 2.1 Load Data\n",
        "\n",
        "Our first step is to load the data and divide it into a training and testing dataset. The MNIST dataset can be downloaded directly from TensorFlow and has already been divided. Run the next cell to import the data.\n",
        "\n",
        "``` x_train ``` is the dataset of 28x28 images of handwritten digits that the model will be trained on.\n",
        "\n",
        "```y_train``` is the dataset of labels that correspond to ```x_train```. \n",
        "\n",
        "``` x_test ``` is the dataset of 28x28 images of handwritten digits that the model will be tested on.\n",
        "\n",
        "```y_test``` is the dataset of labels that correspond to ```x_test```. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8efB13kY6N7g"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-IoQOH6N7g"
      },
      "source": [
        "Run the following code to see the counts of each digit present in our training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "lyuCUAZ_6N7h",
        "outputId": "6ab3bbf0-d108-42f8-c18f-f81856dbc725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f04c28085d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD6CAYAAABQ6WtbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVnElEQVR4nO3df7BfdX3n8edLIlWpNUHSLCZ0w6wZW9pdFe8Alq51zRoCtYZxkMFZNcuyE3cGHV07W7GdWSwsO7rb1qptmclINFiVRtSFOoyYwV/b7vLjBhGB6HJFkWSB3JqIP1i12Pf+8f1EvyT3ci5yz7k33Odj5jvfcz7nc87nfTOBV845n3NuqgpJkh7LUxa6AEnS4mdYSJI6GRaSpE6GhSSpk2EhSepkWEiSOvUWFkmel+S2sc93k7wlybFJdia5u32vaP2T5L1JppLcnuTksWNtbv3vTrK5r5olSTPLEM9ZJDkK2AucClwI7K+qdya5CFhRVW9LchbwJuCs1u89VXVqkmOBSWACKGAX8KKqOjDbeMcdd1ytXbu2159Jkp5sdu3a9fdVtXKmbcsGqmE98PWqujfJJuClrX078HngbcAm4MoapdeNSZYnOb713VlV+wGS7AQ2Ah+dbbC1a9cyOTnZ048iSU9OSe6dbdtQ9yzO42f/c19VVfe35QeAVW15NXDf2D57Wtts7ZKkgfQeFkmOBl4JfOzQbe0sYl6ugyXZkmQyyeT09PR8HFKS1AxxZnEmcGtVPdjWH2yXl2jf+1r7XuCEsf3WtLbZ2h+lqrZW1URVTaxcOeMlN0nSz2mIsHgNj76/cC1wcEbTZuCasfbXt1lRpwEPtctV1wMbkqxoM6c2tDZJ0kB6vcGd5Bjg5cAbxprfCexIcgFwL3Bua7+O0UyoKeBh4HyAqtqf5FLgltbvkoM3uyVJwxhk6uzQJiYmytlQkvT4JNlVVRMzbfMJbklSJ8NCktTJsJAkdRrqCe4l71uX/PPBxvqV//yVwcaStDR4ZiFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6uRbZyUtCu94xzuelGM9WXhmIUnqZFhIkjoZFpKkTt6z0OC+8JLfHmys3/7iFwYbS3oy6/XMIsnyJFcn+WqS3UlenOTYJDuT3N2+V7S+SfLeJFNJbk9y8thxNrf+dyfZ3GfNkqTD9X0Z6j3Ap6vqV4HnA7uBi4AbqmodcENbBzgTWNc+W4DLAZIcC1wMnAqcAlx8MGAkScPoLSySPAt4CXAFQFX9uKq+A2wCtrdu24Gz2/Im4MoauRFYnuR44AxgZ1Xtr6oDwE5gY191S5IO1+eZxYnANPCBJF9K8v4kxwCrqur+1ucBYFVbXg3cN7b/ntY2W7skaSB9hsUy4GTg8qp6IfADfnbJCYCqKqDmY7AkW5JMJpmcnp6ej0NKkpo+Z0PtAfZU1U1t/WpGYfFgkuOr6v52mWlf274XOGFs/zWtbS/w0kPaP3/oYFW1FdgKMDExMS8B9GR0+vtOH2Scv3vT3w0yjvRk9Pyrrx9srC+fc8ac+vUWFlX1QJL7kjyvqr4GrAfuap/NwDvb9zVtl2uBNya5itHN7IdaoFwP/Nexm9obgLc/nlpe9J+ufOI/0Bzs+u+vH2Qcab7tvuyzg4zza3/4skHG0fzr+zmLNwEfTnI0cA9wPqNLXzuSXADcC5zb+l4HnAVMAQ+3vlTV/iSXAre0fpdU1f6e65Ykjek1LKrqNmBihk3rZ+hbwIWzHGcbsG1+q9NS9+e/9zeDjPPGP/ndQcbR/NjxsVMGGefcV988yDjzxdd9SJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6tT3W2clPYbLXnvOYGP94V9dPdhYevLxzEKS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUqdewSPLNJF9JcluSydZ2bJKdSe5u3ytae5K8N8lUktuTnDx2nM2t/91JNvdZsyTpcEOcWfyrqnpBVU209YuAG6pqHXBDWwc4E1jXPluAy2EULsDFwKnAKcDFBwNGkjSMhbgMtQnY3pa3A2ePtV9ZIzcCy5McD5wB7Kyq/VV1ANgJbBy6aElayvoOiwI+k2RXki2tbVVV3d+WHwBWteXVwH1j++5pbbO1P0qSLUkmk0xOT0/P588gSUte32+d/a2q2pvkl4GdSb46vrGqKknNx0BVtRXYCjAxMTEvx5QkjfR6ZlFVe9v3PuCTjO45PNguL9G+97Xue4ETxnZf09pma5ckDaS3sEhyTJJnHlwGNgB3ANcCB2c0bQauacvXAq9vs6JOAx5ql6uuBzYkWdFubG9obZKkgfR5GWoV8MkkB8f5SFV9OsktwI4kFwD3Aue2/tcBZwFTwMPA+QBVtT/JpcAtrd8lVbW/x7olSYfoLSyq6h7g+TO0fxtYP0N7ARfOcqxtwLb5rlGSNDc+wS1J6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnq1HtYJDkqyZeSfKqtn5jkpiRTSf46ydGt/Rfa+lTbvnbsGG9v7V9LckbfNUuSHm2IM4s3A7vH1t8FvLuqngscAC5o7RcAB1r7u1s/kpwEnAf8OrAR+MskRw1QtySp6TUskqwBfgd4f1sP8DLg6tZlO3B2W97U1mnb17f+m4CrqupHVfUNYAo4pc+6JUmP1veZxZ8Bvw/8Y1t/NvCdqnqkre8BVrfl1cB9AG37Q63/T9tn2EeSNIDewiLJK4B9VbWrrzEOGW9Lkskkk9PT00MMKUlLRp9nFqcDr0zyTeAqRpef3gMsT7Ks9VkD7G3Le4ETANr2ZwHfHm+fYZ+fqqqtVTVRVRMrV66c/59Gkpaw3sKiqt5eVWuqai2jG9Sfrap/A3wOOKd12wxc05avbeu07Z+tqmrt57XZUicC64Cb+6pbknS4OYVFkhvm0jZHbwPemmSK0T2JK1r7FcCzW/tbgYsAqupOYAdwF/Bp4MKq+snPObYk6eew7LE2Jnka8AzguCQrgLRNv8TjuMlcVZ8HPt+W72GG2UxV9UPg1bPsfxlw2VzHkyTNr8cMC+ANwFuA5wC7+FlYfBf48x7rkiQtIo8ZFlX1HuA9Sd5UVe8bqCZJ0iLTdWYBQFW9L8lvAmvH96mqK3uqS5K0iMwpLJJ8CPhnwG3AwZvLBRgWkrQEzCksgAngpDaVVZK0xMz1OYs7gH/SZyGSpMVrrmcWxwF3JbkZ+NHBxqp6ZS9VSZIWlbmGxTv6LEKStLjNdTbUF/ouRJK0eM11NtT3GM1+AjgaeCrwg6r6pb4KkyQtHnM9s3jmweWxX0h0Wl9FSZIWl8f91tka+R+AvwtbkpaIuV6GetXY6lMYPXfxw14qkiQtOnOdDfW7Y8uPAN9kdClKkrQEzPWexfl9FyJJWrzm+suP1iT5ZJJ97fPxJGv6Lk6StDjM9Qb3Bxj9etPntM/ftDZJ0hIw17BYWVUfqKpH2ueDwMoe65IkLSJzDYtvJ3ltkqPa57XAt/ssTJK0eMw1LP4dcC7wAHA/cA7wb3uqSZK0yMx16uwlwOaqOgCQ5FjgjxmFiCTpSW6uZxb/4mBQAFTVfuCF/ZQkSVps5hoWT0my4uBKO7N4zLOSJE9LcnOSLye5M8kftfYTk9yUZCrJXyc5urX/QlufatvXjh3r7a39a0l8zYgkDWyuYfEnwP9OcmmSS4H/Bfy3jn1+BLysqp4PvADYmOQ04F3Au6vqucAB4ILW/wLgQGt/d+tHkpOA84BfBzYCf5nkqLn+gJKkJ25OYVFVVwKvAh5sn1dV1Yc69qmq+n5bfWr7FPAy4OrWvh04uy1vauu07evH3nB7VVX9qKq+AUwBp8ylbknS/JjrDW6q6i7grsdz8HYGsAt4LvAXwNeB71TVI63LHmB1W14N3NfGeiTJQ8CzW/uNY4cd30eSNIDH/Yryx6OqflJVLwDWMDob+NW+xkqyJclkksnp6em+hpGkJanXsDioqr4DfA54MbA8ycEzmjXA3ra8FzgBoG1/FqMH/37aPsM+42NsraqJqppYudKHyyVpPvUWFklWJlnelp8OvBzYzSg0zmndNgPXtOVr2zpt+2erqlr7eW221InAOuDmvuqWJB1uzvcsfg7HA9vbfYunADuq6lNJ7gKuSvJfgC8BV7T+VwAfSjIF7Gc0A4qqujPJDkb3Sx4BLqyqn/RYtyTpEL2FRVXdzgwP7lXVPcwwm6mqfgi8epZjXQZcNt81SpLmZpB7FpKkI5thIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSerUW1gkOSHJ55LcleTOJG9u7ccm2Znk7va9orUnyXuTTCW5PcnJY8fa3PrfnWRzXzVLkmbW55nFI8DvVdVJwGnAhUlOAi4CbqiqdcANbR3gTGBd+2wBLodRuAAXA6cCpwAXHwwYSdIweguLqrq/qm5ty98DdgOrgU3A9tZtO3B2W94EXFkjNwLLkxwPnAHsrKr9VXUA2Als7KtuSdLhBrlnkWQt8ELgJmBVVd3fNj0ArGrLq4H7xnbb09pmaz90jC1JJpNMTk9Pz2v9krTU9R4WSX4R+Djwlqr67vi2qiqg5mOcqtpaVRNVNbFy5cr5OKQkqek1LJI8lVFQfLiqPtGaH2yXl2jf+1r7XuCEsd3XtLbZ2iVJA+lzNlSAK4DdVfWnY5uuBQ7OaNoMXDPW/vo2K+o04KF2uep6YEOSFe3G9obWJkkayLIej3068DrgK0lua21/ALwT2JHkAuBe4Ny27TrgLGAKeBg4H6Cq9ie5FLil9bukqvb3WLck6RC9hUVV/S2QWTavn6F/ARfOcqxtwLb5q06S9Hj4BLckqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpU29hkWRbkn1J7hhrOzbJziR3t+8VrT1J3ptkKsntSU4e22dz6393ks191StJml2fZxYfBDYe0nYRcENVrQNuaOsAZwLr2mcLcDmMwgW4GDgVOAW4+GDASJKG01tYVNUXgf2HNG8Ctrfl7cDZY+1X1siNwPIkxwNnADuran9VHQB2cngASZJ6NvQ9i1VVdX9bfgBY1ZZXA/eN9dvT2mZrlyQNaMFucFdVATVfx0uyJclkksnp6en5OqwkieHD4sF2eYn2va+17wVOGOu3prXN1n6YqtpaVRNVNbFy5cp5L1ySlrKhw+Ja4OCMps3ANWPtr2+zok4DHmqXq64HNiRZ0W5sb2htkqQBLevrwEk+CrwUOC7JHkazmt4J7EhyAXAvcG7rfh1wFjAFPAycD1BV+5NcCtzS+l1SVYfeNJck9ay3sKiq18yyaf0MfQu4cJbjbAO2zWNpkqTHySe4JUmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSpyMmLJJsTPK1JFNJLlroeiRpKTkiwiLJUcBfAGcCJwGvSXLSwlYlSUvHEREWwCnAVFXdU1U/Bq4CNi1wTZK0ZBwpYbEauG9sfU9rkyQNIFW10DV0SnIOsLGq/n1bfx1walW9cazPFmBLW30e8LUnOOxxwN8/wWPMh8VQx2KoARZHHdbwM4uhjsVQAyyOOuajhn9aVStn2rDsCR54KHuBE8bW17S2n6qqrcDW+RowyWRVTczX8Y7kOhZDDYulDmtYXHUshhoWSx1913CkXIa6BViX5MQkRwPnAdcucE2StGQcEWcWVfVIkjcC1wNHAduq6s4FLkuSlowjIiwAquo64LoBh5y3S1pP0GKoYzHUAIujDmv4mcVQx2KoARZHHb3WcETc4JYkLawj5Z6FJGkBGRYzWOhXiyTZlmRfkjuGHvuQOk5I8rkkdyW5M8mbF6CGpyW5OcmXWw1/NHQNY7UcleRLST61gDV8M8lXktyWZHIB61ie5OokX02yO8mLBx7/ee3P4ODnu0neMmQNrY7/2P5e3pHko0meNnQNrY43txru7OvPwctQh2ivFvk/wMsZPfx3C/CaqrprwBpeAnwfuLKqfmOocWeo43jg+Kq6NckzgV3A2QP/WQQ4pqq+n+SpwN8Cb66qG4eqYayWtwITwC9V1SuGHr/V8E1goqoWdE5/ku3A/6yq97cZis+oqu8sUC1HMZpKf2pV3TvguKsZ/X08qar+X5IdwHVV9cGhamh1/Aajt1qcAvwY+DTwH6pqaj7H8czicAv+apGq+iKwf8gxZ6nj/qq6tS1/D9jNwE/O18j32+pT22fwf+EkWQP8DvD+ocdebJI8C3gJcAVAVf14oYKiWQ98fcigGLMMeHqSZcAzgP+7ADX8GnBTVT1cVY8AXwBeNd+DGBaH89UiM0iyFnghcNMCjH1UktuAfcDOqhq8BuDPgN8H/nEBxh5XwGeS7GpvLVgIJwLTwAfaZbn3JzlmgWqB0XNXHx160KraC/wx8C3gfuChqvrM0HUAdwD/MsmzkzwDOItHP8Q8LwwLdUryi8DHgbdU1XeHHr+qflJVL2D05P4p7bR7MEleAeyrql1DjjuL36qqkxm9gfnCdslyaMuAk4HLq+qFwA+ABfm1Ae0S2CuBjy3A2CsYXXU4EXgOcEyS1w5dR1XtBt4FfIbRJajbgJ/M9ziGxeE6Xy2ylLT7BB8HPlxVn1jIWtqljs8BGwce+nTgle1+wVXAy5L81cA1AD/91yxVtQ/4JKPLpkPbA+wZO8O7mlF4LIQzgVur6sEFGPtfA9+oqumq+gfgE8BvLkAdVNUVVfWiqnoJcIDRfdd5ZVgczleLNO3m8hXA7qr60wWqYWWS5W356YwmHnx1yBqq6u1Vtaaq1jL6+/DZqhr8X5BJjmkTDWiXfTYwugQxqKp6ALgvyfNa03pgsEkPh3gNC3AJqvkWcFqSZ7T/VtYzuq83uCS/3L5/hdH9io/M9xhHzBPcQ1kMrxZJ8lHgpcBxSfYAF1fVFUPW0JwOvA74SrtnAPAH7Wn6oRwPbG8zXp4C7KiqBZu6usBWAZ8c/X+JZcBHqurTC1TLm4APt39Q3QOcP3QBLTBfDrxh6LEBquqmJFcDtwKPAF9i4Z7k/niSZwP/AFzYx4QDp85Kkjp5GUqS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUqf/D/lTeRWqSG9oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.countplot(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlWna8LC6N7j"
      },
      "source": [
        "There are similar counts for each digit. This is good as the model will have enough images for each class to train the features for each class. There is no need to downsample or upweigh."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdpy9Xw06N7j"
      },
      "source": [
        "## 2.2 Check for NaN Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmlXkZxo6N7k",
        "outputId": "6d2e23a2-90b0-4ca7-d12b-934c879ff542"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "np.isnan(x_train).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgQPIArD6N7k",
        "outputId": "043be6c9-9dff-4c15-8ce8-98847f1deac5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "np.isnan(x_test).any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xye138C6N7l"
      },
      "source": [
        "There are no NaN values in our dataset. There is no need to preprocess the data to deal with Nan's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHQ4yLj_6N7l"
      },
      "source": [
        "## 2.3 Normalization and Reshaping\n",
        "\n",
        "Since the values in our ```x_train``` dataset are 28x28 images, our input shape must be specified so that our model will know what is being inputed.\n",
        "\n",
        "The first convolution layer expects a single 60000x28x28x1 tensor instead of 60000 28x28x1 tensors.\n",
        "\n",
        "Models generally run better on normalized values. The best way to normalize the data depends on each individual dataset. For the MNIST dataset, we want each value to be between 0.0 and 1.0. As all values originally fall under the 0.0-255.0 range, divide by 255.0.\n",
        "\n",
        "Run the following cell to define the ```input_shape``` and to normalize and reshape the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV3cgXgP6N7m"
      },
      "outputs": [],
      "source": [
        "input_shape = (28, 28, 1)\n",
        "\n",
        "x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "x_train=x_train / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
        "x_test=x_test/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOOTjHLy6N7m"
      },
      "source": [
        "## 2.4 Label Encoding\n",
        "\n",
        "The labels for the training and the testing dataset are currently categorical and is not continuous. To include categorical dataset in our model, our labels should be converted to one-hot encodings.\n",
        "\n",
        "For example, ```2``` becomes ```[0,0,1,0,0,0,0,0,0,0]``` and ```7``` becomes ```[0,0,0,0,0,0,0,1,0,0]```.\n",
        "\n",
        "Run the following cell to transform the labels into one-hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N60OSnQM6N7n"
      },
      "outputs": [],
      "source": [
        "y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
        "y_test = tf.one_hot(y_test.astype(np.int32), depth=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_VT_27H6N7n"
      },
      "source": [
        "## 2.5 Visualize Data\n",
        "\n",
        "Run the following cell to visualize an image in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "0dMzhGd16N7o",
        "outputId": "800b27d5-226f-4c1d-c91d-441aee7738f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(x_train[100][:,:,0])\n",
        "print(y_train[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDAQvbMv6N7o"
      },
      "source": [
        "The image is an image of a handwritten ```5```. The one-hot encoding holds the value of ```5```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkI6R7p_6N7p"
      },
      "source": [
        "# 3. CNN\n",
        "\n",
        "In this module, we will build our CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwf3fx-s6N7p"
      },
      "source": [
        "## 3.1 Define the Model\n",
        "\n",
        "Run the following cell to define ```batch_size```, ```num_classes```, and ```epochs```. Try changing the values and test how different values affect the accuracy of the CNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BSgPmfd6N7p"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDPl3f8c6N7q"
      },
      "source": [
        "Run the following cell to build the model. The model contains various layers stacked on top of each other. The output of one layer feeds into the input of the next layer.\n",
        "\n",
        "Conv2D layers are convolutions. Each filter (32 in the first two convolution layers and 64 in the next two convolution layers) transforms a part of the image (5x5 for the first two Conv2D layers and 3x3 for the next two Conv2D layers). The transformation is applied on the whole image.\n",
        "\n",
        "MaxPool2D is a downsampling filter. It reduces a 2x2 matrix of the image to a single pixel with the maximum value of the 2x2 matrix. The filter aims to conserve the main features of the image while reducing the size.\n",
        "\n",
        "Dropout is a regularization layer. In our model, 25% of the nodes in the layer are randomly ignores, allowing the network to learn different features. This prevents overfitting.\n",
        "\n",
        "```relu``` is the rectifier, and it is used to find nonlinearity in the data. It works by returning the input value if the input value >= 0. If the input is negative, it returns 0.\n",
        "\n",
        "Flatten converts the tensors into a 1D vector.\n",
        "\n",
        "The Dense layers are an artificial neural network (ANN). The last layer returns the probability that an image is in each class (one for each digit).\n",
        "\n",
        "As this model aims to categorize the images, we will use a ```categorical_crossentropy``` loss function. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def activationExcitatory(x):\n",
        "    return K.maximum(x, 0)\n",
        "\n",
        "def activationInhibitory(x):\n",
        "    return -(K.maximum(x, 0))\n",
        "\n",
        "def excitatoryNeuronInitializer(shape, dtype=None):\n",
        "    return tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "\n",
        "def inhibitoryNeuronInitializer(shape, dtype=None):\n",
        "    return tf.math.negative(tf.math.abs(tf.random.normal(shape, dtype=dtype)))\n"
      ],
      "metadata": {
        "id": "G_TJH-X4qhtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLN-jkxg6N7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f319926-8460-41b8-82af-1031d34f0b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 28, 28, 1600  41600       ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 28, 28, 1600  41600       ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 28, 28, 1600  0           ['conv2d[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 28, 28, 1600  0           ['conv2d_1[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 28, 28, 3200  0           ['activation[0][0]',             \n",
            "                                )                                 'activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 28, 28, 1600  128001600   ['concatenate[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 28, 28, 1600  128001600   ['concatenate[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 28, 28, 1600  0           ['conv2d_2[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 28, 28, 1600  0           ['conv2d_3[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 28, 28, 3200  0           ['activation_2[0][0]',           \n",
            "                                )                                 'activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 14, 14, 3200  0           ['concatenate_1[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 14, 14, 3200  0           ['max_pooling2d[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 627200)       0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 627200)       0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           6272010     ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 262,358,410\n",
            "Trainable params: 262,358,410\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "numberOfHiddenLayers = 2  #5  #default = 5, if 0 then useSVM=True\n",
        "generateLargeNetworkUntrained = True\n",
        "addSkipLayers = False\n",
        "if(numberOfHiddenLayers > 1):\n",
        "  addSkipLayers = False #True  #optional\n",
        "\n",
        "if(generateLargeNetworkUntrained):\n",
        "  generateNetworkUntrained = True\n",
        "  largeNetworkRatio = 50  #100\n",
        "  generateLargeNetworkExpansion = False\n",
        "  if(generateLargeNetworkExpansion):\n",
        "    generateLargeNetworkRatioExponential = False\n",
        "else:\n",
        "  generateNetworkUntrained = False\n",
        "  generateLargeNetworkRatio = False\n",
        "\n",
        "def getLayerRatio(layerIndex):\n",
        "  layerRatio = 1\n",
        "  if(generateLargeNetworkUntrained):\n",
        "    if(generateLargeNetworkExpansion):\n",
        "      if(generateLargeNetworkRatioExponential):\n",
        "        layerRatio = largeNetworkRatio**layerIndex\n",
        "      else:\n",
        "        layerRatio = largeNetworkRatio * layerIndex\n",
        "    else:\n",
        "      layerRatio = largeNetworkRatio\n",
        "  else:\n",
        "    layerRatio = 1\n",
        "  return int(layerRatio)\n",
        "\n",
        "x = tf.keras.layers.Input(shape=input_shape)\n",
        "hLast = x\n",
        "if(numberOfHiddenLayers >= 1):\n",
        "  layerRatio = getLayerRatio(1)\n",
        "  h1E = tf.keras.layers.Conv2D(32*layerRatio, (5,5), kernel_initializer=excitatoryNeuronInitializer, padding='same')(x)\n",
        "  h1I = tf.keras.layers.Conv2D(32*layerRatio, (5,5), kernel_initializer=inhibitoryNeuronInitializer, padding='same')(x)\n",
        "  #print(\"h1E.shape = \", h1E.shape)\n",
        "  h1E = tf.keras.layers.Activation(activationExcitatory)(h1E)\n",
        "  h1I = tf.keras.layers.Activation(activationInhibitory)(h1I)\n",
        "  h1 = tf.keras.layers.Concatenate()([h1E, h1I])\n",
        "  #print(\"h1E.shape = \", h1E.shape)\n",
        "  #print(\"h1.shape = \", h1.shape)\n",
        "  hLast = h1\n",
        "if(numberOfHiddenLayers >= 2):\n",
        "  layerRatio = getLayerRatio(2)\n",
        "  h2E = tf.keras.layers.Conv2D(32*layerRatio, (5,5), kernel_initializer=excitatoryNeuronInitializer, padding='same')(h1)\n",
        "  h2I = tf.keras.layers.Conv2D(32*layerRatio, (5,5), kernel_initializer=inhibitoryNeuronInitializer, padding='same')(h1)\n",
        "  h2E = tf.keras.layers.Activation(activationExcitatory)(h2E)\n",
        "  h2I = tf.keras.layers.Activation(activationInhibitory)(h2I)\n",
        "  h2 = tf.keras.layers.Concatenate()([h2E, h2I])\n",
        "  h2 = tf.keras.layers.MaxPool2D()(h2)\n",
        "  h2 = tf.keras.layers.Dropout(0.25)(h2)\n",
        "  hLast = h2\n",
        "if(numberOfHiddenLayers >= 3):\n",
        "  layerRatio = getLayerRatio(3)\n",
        "  h3E = tf.keras.layers.Conv2D(32*layerRatio, (3,3), kernel_initializer=excitatoryNeuronInitializer, padding='same')(h2)\n",
        "  h3I = tf.keras.layers.Conv2D(32*layerRatio, (3,3), kernel_initializer=inhibitoryNeuronInitializer, padding='same')(h2)\n",
        "  h3E = tf.keras.layers.Activation(activationExcitatory)(h3E)\n",
        "  h3I = tf.keras.layers.Activation(activationInhibitory)(h3I)\n",
        "  h3 = tf.keras.layers.Concatenate()([h3E, h3I])\n",
        "  hLast = h3\n",
        "if(numberOfHiddenLayers >= 4):\n",
        "  layerRatio = getLayerRatio(4)\n",
        "  h4E = tf.keras.layers.Conv2D(32*layerRatio, (3,3), kernel_initializer=excitatoryNeuronInitializer, padding='same')(h3)\n",
        "  h4I = tf.keras.layers.Conv2D(32*layerRatio, (3,3), kernel_initializer=inhibitoryNeuronInitializer, padding='same')(h3)\n",
        "  h4E = tf.keras.layers.Activation(activationExcitatory)(h4E)\n",
        "  h4I = tf.keras.layers.Activation(activationInhibitory)(h4I)\n",
        "  h4 = tf.keras.layers.Concatenate()([h4E, h4I])\n",
        "  h4 = tf.keras.layers.MaxPool2D(strides=(2,2))(h4)\n",
        "  h4 = tf.keras.layers.Flatten()(h4)\n",
        "  hLast = h4\n",
        "if(numberOfHiddenLayers >= 5):\n",
        "  layerRatio = getLayerRatio(5)\n",
        "  h5E = tf.keras.layers.Dense(128*layerRatio, kernel_initializer=excitatoryNeuronInitializer)(h4)\n",
        "  h5I = tf.keras.layers.Dense(128*layerRatio, kernel_initializer=inhibitoryNeuronInitializer)(h4)\n",
        "  h5E = tf.keras.layers.Activation(activationExcitatory)(h5E)\n",
        "  h5I = tf.keras.layers.Activation(activationInhibitory)(h5I)\n",
        "  h5 = tf.keras.layers.Concatenate()([h5E, h5I])\n",
        "  h5 = tf.keras.layers.Dropout(0.5)(h5)\n",
        "  hLast = h5\n",
        "if(addSkipLayers):\n",
        "  mList = []\n",
        "  if(numberOfHiddenLayers >= 1):\n",
        "    m1 = tf.keras.layers.Flatten()(h1)\n",
        "    mList.append(m1)\n",
        "  if(numberOfHiddenLayers >= 2):\n",
        "    m2 = tf.keras.layers.Flatten()(h2)\n",
        "    mList.append(m2)\n",
        "  if(numberOfHiddenLayers >= 3):\n",
        "    m3 = tf.keras.layers.Flatten()(h3)\n",
        "    mList.append(m3)\n",
        "  if(numberOfHiddenLayers >= 4):\n",
        "    m4 = tf.keras.layers.Flatten()(h4)\n",
        "    mList.append(m4)\n",
        "  if(numberOfHiddenLayers >= 5):\n",
        "    m5 = h5\n",
        "    mList.append(m5)\n",
        "  hLast = tf.keras.layers.concatenate(mList)\n",
        "hLast = tf.keras.layers.Flatten()(hLast)  #flatten hLast if necessary (ie numberOfHiddenLayers <4)\n",
        "if(generateNetworkUntrained):\n",
        "  hLast = tf.keras.layers.Lambda(lambda x: tf.keras.backend.stop_gradient(x))(hLast)\n",
        "y = tf.keras.layers.Dense(num_classes, activation='softmax')(hLast)\n",
        "model = tf.keras.Model(x, y)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDeOzJFI6N7r"
      },
      "source": [
        "## 3.2 Fit the Training Data\n",
        "\n",
        "The next step is to fit our training data. If we achieve a certain level of accuracy, it may not be necessary to continue training the model, especially if time and resources are limited.\n",
        "\n",
        "The following cell defines a CallBack so that if 99.5% accuracy is achieved, the model stops training. The model is not likely to stop prematurely if only 5 epochs are specified. Try it out with more epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM1t_AIX6N7r"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>0.995):\n",
        "      print(\"\\nReached 99.5% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3-Edk1Q6N7s"
      },
      "source": [
        "Testing the model on a validation dataset prevents overfitting of the data. We specified a 10% validation and 90% training split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9ocCw_T6N7s",
        "outputId": "4ea7d7ea-8b88-4449-cb6c-3896a5622a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            " 55/844 [>.............................] - ETA: 85:44:21 - loss: 18992624.0000 - acc: 0.4662"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[callbacks])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr34Iqnn6N7s"
      },
      "source": [
        "# 4. Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqvc6ES76N7s"
      },
      "source": [
        "## 4.1 Loss and Accuracy Curves\n",
        "\n",
        "Run the following cell to evaluate the loss and accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuCQbNgW6N7s"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2,1)\n",
        "ax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\n",
        "ax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\",axes =ax[0])\n",
        "legend = ax[0].legend(loc='best', shadow=True)\n",
        "\n",
        "ax[1].plot(history.history['acc'], color='b', label=\"Training Accuracy\")\n",
        "ax[1].plot(history.history['val_acc'], color='r',label=\"Validation Accuracy\")\n",
        "legend = ax[1].legend(loc='best', shadow=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5ChJfLQ6N7u"
      },
      "source": [
        "The accuracy increases over time and the loss decreases over time. However, the accuracy of our validation set seems to slightly decrease towards the end even thought our training accuracy increased. Running the model for more epochs might cause our model to be susceptible to overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6JiUyTe6N7u"
      },
      "source": [
        "## 4.2 Predict Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jz4oEW06N7u"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dozTBo5G6N7v"
      },
      "source": [
        "Our model runs pretty well, with an accuracy of 99.3% on our testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg6DZgcv6N7v"
      },
      "source": [
        "## 4.3 Confusion Matrix\n",
        "\n",
        "Run the following cell to compute our confusion matrix using TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EUqmdvq6N7v"
      },
      "outputs": [],
      "source": [
        "# Predict the values from the testing dataset\n",
        "Y_pred = model.predict(x_test)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert testing observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1)\n",
        "# compute the confusion matrix\n",
        "confusion_mtx = tf.math.confusion_matrix(Y_true, Y_pred_classes) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZjDurLZ6N7v"
      },
      "source": [
        "Run the following cell to plot the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ytj2AMX6N7w"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx, annot=True, fmt='g')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7AQaxdS6N7w"
      },
      "source": [
        "There seems to be a slightly higher confusion between (0,6) and (4,9). This is reasonable as 0's and 6's look similar with their loops and 4's and 9's can be mistaken when the 4's are more rounded and 9's are more angular."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MNISTlargeUntrainedNetCNNexcInh.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}