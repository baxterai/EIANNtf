{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wF5wszaj97Y"
      },
      "source": [
        "# MNIST Large Untrained Net Exc Inh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04QgGZc9bF5D"
      },
      "source": [
        "Derived from https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0trJmd6DjqBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a58e97e4-2041-41da-8fa0-e4f0830a3c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "from keras import backend as K\n",
        "#print(\"keras version:\",tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "import math"
      ],
      "metadata": {
        "id": "36nIv8fWBeuV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inhibitoryNeuronOutputPositive = True\n",
        "if(inhibitoryNeuronOutputPositive):\n",
        "    inhibitoryNeuronSwitchActivation = True\n",
        "else:\n",
        "    inlineImplementation = False\t#orig: True #True: excitatory/inhibitory neurons are on same sublayer, False: add inhibitory neurons to separate preceding sublayer\n",
        "    if(inlineImplementation):\n",
        "        positiveWeightImplementation = False\t#orig: True #optional\n",
        "        if(not positiveWeightImplementation):\n",
        "            integrateWeights = True    #orig: False #optional\n",
        "            if(integrateWeights):\n",
        "                integrateWeights1 = False    #explicitly declare E/I neurons\n",
        "                integrateWeights2 = True    #implicitly declare E/I neurons\n",
        "                integrateWeightsInitialiseZero = False   #miscellaneous training performance improvement (single EI layer only)\n",
        "        excitatoryNeuronThreshold = 0.0 #mandatory\n",
        "    else:\n",
        "        positiveWeightImplementation = False    #False: only current coded implementation\n",
        "        inhibitoryNeuronNormalisationFactorStatic = False    #True: normalise intermediary inhibitory neuron layer based on h0/h1 num neurons, False: normalise based on h0/h1 activations\n",
        "        excitatoryNeuronThreshold = 0.0   #orig: 0.0\n",
        "\n",
        "inputLayerExcitatoryOnly = True #True: only current coded implementation\n",
        "\n",
        "generateUntrainedNetwork = False\n",
        "if(generateUntrainedNetwork):\n",
        "    #only train the last layer\n",
        "    numberOfHiddenLayers = 2    #default: 2    #if 0 then useSVM=True\n",
        "else:\n",
        "    numberOfHiddenLayers = 2 #default: 2\n",
        "\n",
        "if(numberOfHiddenLayers > 1):\n",
        "    addSkipLayers = False   #optional\n",
        "else:\n",
        "    addSkipLayers = False   #mandatory\n",
        "\n",
        "layerSizeBase = 128  #default: 128\n",
        "\n",
        "batch_size = 64 #default: 64\n",
        "epochs = 5  #100  #1  #5\n",
        "\n",
        "debugNoEIneurons = False\n",
        "debugPreTrainWeights = True\n",
        "debugPreTrainOutputs = True\n",
        "debugPostTrainWeights = True\n",
        "debugPostTrainOutputs = True"
      ],
      "metadata": {
        "id": "cq_TphnxNEe8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGfQXr--YeL7"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7FP5258xjs-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f5a23f-fbb1-40b8-8076-810c57ea5249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train.shape =  (60000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "print(\"x_train.shape = \", x_train.shape)\n",
        "\n",
        "input_shape = (28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10"
      ],
      "metadata": {
        "id": "29RfPJhqwngo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EIactivation(Z):\n",
        "    A = K.maximum(Z, excitatoryNeuronThreshold)-excitatoryNeuronThreshold  #ReLU\n",
        "    return A\n",
        "\n",
        "def EIactivationExcitatory(Z):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            return EIactivation(Z)\n",
        "        else:\n",
        "             print(\"EIactivationExcitatory error: requires positiveWeightImplementation\")\n",
        "    else:\n",
        "        print(\"EIactivationExcitatory error: requires inlineImplementation\")\n",
        "\n",
        "def EIactivationInhibitory(Z):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            return -EIactivation(Z)   #ReLU with negative output\n",
        "        else:\n",
        "             print(\"EIactivationInhibitory error: requires positiveWeightImplementation\")\n",
        "    else:\n",
        "        print(\"inlineImplementation error: requires inlineImplementation\")\n",
        "\n",
        "def EIweightInitializer(shape, dtype=None):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            w = tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "        else:\n",
        "            if(integrateWeights):\n",
        "                if(integrateWeightsInitialiseZero):\n",
        "                    w = tf.random.normal(shape, dtype=dtype)\n",
        "                    #w = tf.zeros(shape, dtype=dtype)    #tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "                else:\n",
        "                    w = tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "                    wEIsize = w.shape[0]//2\n",
        "                    wSignE = tf.ones([wEIsize, w.shape[1]])\n",
        "                    wSignI = tf.ones([wEIsize, w.shape[1]])\n",
        "                    wSignI = tf.multiply(wSignI, -1)\n",
        "                    wSign = tf.concat([wSignE, wSignI], axis=0)\n",
        "                    w = tf.multiply(w, wSign)\n",
        "            else:\n",
        "                print(\"EIweightInitializer error: requires !positiveWeightImplementation:integrateWeights\")\n",
        "    else:\n",
        "        print(\"EIweightInitializer error: requires inlineImplementation\")\n",
        "\n",
        "    return w\n",
        "\n",
        "def EIweightInitializerExcitatory(shape, dtype=None):\n",
        "    if(positiveWeightImplementation):\n",
        "        print(\"EIweightInitializerExcitatory error: requires !positiveWeightImplementation\")\n",
        "    else:\n",
        "        return tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "\n",
        "def EIweightInitializerInhibitory(shape, dtype=None):\n",
        "    if(positiveWeightImplementation):\n",
        "        print(\"EIweightInitializerExcitatory error: requires !positiveWeightImplementation\")\n",
        "    else:\n",
        "        return tf.math.negative(tf.math.abs(tf.random.normal(shape, dtype=dtype)))\n",
        "\n",
        "def EIweightInitialisedAverage(shape):\n",
        "    return tf.reduce_mean(tf.math.abs(tf.random.normal(shape)))\n",
        "\n",
        "class negative(tf.keras.constraints.Constraint):\n",
        "    #based on https://www.tensorflow.org/api_docs/python/tf/keras/constraints/Constraint\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, w):\n",
        "        return w * tf.cast(tf.math.less_equal(w, 0.), w.dtype)\n",
        "\n",
        "class positiveOrNegative(tf.keras.constraints.Constraint):\n",
        "    #based on https://www.tensorflow.org/api_docs/python/tf/keras/constraints/Constraint\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, w):\n",
        "        w_shape = w.shape\n",
        "        wEIsize = w.shape[0]//2\n",
        "        wE = w[0:wEIsize]\n",
        "        wI = w[wEIsize:]\n",
        "        wEcheck = tf.greater_equal(wE, 0)\n",
        "        wIcheck = tf.less_equal(wI, 0)\n",
        "        wEcheck = tf.cast(wEcheck, tf.float32)\n",
        "        wIcheck = tf.cast(wIcheck, tf.float32)\n",
        "        wE = tf.multiply(wE, wEcheck)\n",
        "        wI = tf.multiply(wI, wIcheck)\n",
        "        w = tf.concat([wE, wI], axis=0)\n",
        "        return w\n"
      ],
      "metadata": {
        "id": "bcEe88PrSf5h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not inhibitoryNeuronOutputPositive):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            EIweightConstraint = tf.keras.constraints.non_neg()\n",
        "            constrainBiases = True   #ensure positive biases also\n",
        "            if(constrainBiases):\n",
        "                EIbiasConstraint = tf.keras.constraints.non_neg()\n",
        "                constrainBiasesLastLayer = False\n",
        "                if(constrainBiasesLastLayer):\n",
        "                    EIbiasConstraintLastLayer = tf.keras.constraints.non_neg()\n",
        "                else:\n",
        "                    EIbiasConstraintLastLayer = None\n",
        "            else:\n",
        "                EIbiasConstraint = None\n",
        "                EIbiasConstraintLastLayer = None\n",
        "            EIweightConstraintLastLayer = EIweightConstraint\n",
        "        else:\n",
        "            if(integrateWeights):\n",
        "                EIweightConstraint = positiveOrNegative()\n",
        "                EIbiasConstraint = None\n",
        "                EIweightConstraintLastLayer = None\n",
        "                EIbiasConstraintLastLayer = None\n",
        "            else:\n",
        "                EIweightConstraintPositive = tf.keras.constraints.non_neg()\n",
        "                EIweightConstraintNegative = negative()\n",
        "                constrainBiases = False\n",
        "                if(constrainBiases):\n",
        "                    EIbiasConstraintPositive = tf.keras.constraints.non_neg()\n",
        "                    EIbiasConstraintNegative = negative()\n",
        "                else:\n",
        "                    EIbiasConstraintPositive = None\n",
        "                    EIbiasConstraintNegative = None\n",
        "                EIweightConstraintLastLayer = None\n",
        "                EIbiasConstraintLastLayer = None\n",
        "    else:\n",
        "        EIweightConstraintPositive = tf.keras.constraints.non_neg()\n",
        "        EIweightConstraintNegative = negative()\n",
        "        constrainBiases = False\n",
        "        if(constrainBiases):\n",
        "            EIbiasConstraintPositive = tf.keras.constraints.non_neg()\n",
        "            EIbiasConstraintNegative = negative()\n",
        "        else:\n",
        "            EIbiasConstraintPositive = None\n",
        "            EIbiasConstraintNegative = None\n",
        "        EIweightConstraintLastLayer = None\n",
        "        EIbiasConstraintLastLayer = None\n",
        "\n",
        "if(generateUntrainedNetwork):\n",
        "    #only train the last layer\n",
        "    generateLargeNetwork = True\n",
        "else:\n",
        "    generateLargeNetwork = False\n",
        "\n",
        "\n",
        "if(generateLargeNetwork):\n",
        "    generateLargeNetworkRatio = 50\n",
        "    layerRatio = generateLargeNetworkRatio\n",
        "else:\n",
        "    layerRatio = 1  #10 #1\n",
        "\n",
        "def createEIlayer(layerIndex, h0, numChannels, previousNumChannels, firstLayer=False):\n",
        "    if(debugNoEIneurons):\n",
        "        h1 = tf.keras.layers.Dense(numChannels)(h0)\n",
        "        h1 = tf.keras.layers.Activation(EIactivation)(h1)\n",
        "    else:\n",
        "        if(inhibitoryNeuronOutputPositive):\n",
        "            h1E = tf.keras.layers.Dense(numChannels)(h0)\n",
        "            h1I = tf.keras.layers.Dense(numChannels)(h0)\n",
        "            h1E = tf.keras.layers.Activation('relu')(h1E)\n",
        "            if(inhibitoryNeuronSwitchActivation):\n",
        "                h1I = tf.keras.layers.Activation('relu')(-h1I)\n",
        "            else:\n",
        "                h1I = tf.keras.layers.Activation('relu')(h1I)\n",
        "            h1 = tf.keras.layers.Concatenate()([h1E, h1I])\n",
        "        else:\n",
        "            if(inlineImplementation):\n",
        "                if(positiveWeightImplementation):\n",
        "                    h1E = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)\n",
        "                    h1I = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)\n",
        "                    h1E = tf.keras.layers.Activation(EIactivationExcitatory)(h1E)\n",
        "                    h1I = tf.keras.layers.Activation(EIactivationInhibitory)(h1I)\n",
        "                    h1 = tf.keras.layers.Concatenate()([h1E, h1I])\n",
        "                else:\n",
        "                    if(integrateWeights):\n",
        "                        if(integrateWeights1):\n",
        "                            if(firstLayer):\n",
        "                                h1E = tf.keras.layers.Dense(numChannels)(h0)   #excitatory neuron inputs\n",
        "                                h1I = tf.keras.layers.Dense(numChannels)(h0)   #inhibitory neuron inputs\n",
        "                            else:\n",
        "                                h1E = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)   #excitatory neuron inputs\n",
        "                                h1I = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)   #inhibitory neuron inputs\n",
        "                            h1E = tf.keras.layers.Activation(EIactivation)(h1E)\n",
        "                            h1I = tf.keras.layers.Activation(EIactivation)(h1I)\n",
        "                            h1 = tf.keras.layers.Concatenate()([h1E, h1I])\n",
        "                        elif(integrateWeights2):\n",
        "                            if(firstLayer):\n",
        "                                h1 = tf.keras.layers.Dense(numChannels*2)(h0)   #excitatory neuron inputs   #OLD:*4\n",
        "                            else:\n",
        "                                h1 = tf.keras.layers.Dense(numChannels*2, kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)   #excitatory neuron inputs\n",
        "                            h1 = tf.keras.layers.Activation(EIactivation)(h1) #ReLU\n",
        "                    else:\n",
        "                        if(firstLayer):\n",
        "                            h1E = tf.keras.layers.Dense(numChannels)(h0)   #excitatory neuron inputs\n",
        "                            h1I = tf.keras.layers.Dense(numChannels)(h0)   #inhibitory neuron inputs\n",
        "                        else:\n",
        "                            h0E, h0I = h0\n",
        "                            h1Ee = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0E) #excitatory neuron excitatory inputs\n",
        "                            h1Ei = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h0I) #excitatory neuron inhibitory inputs\n",
        "                            h1Ie = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0E) #inhibitory neuron excitatory inputs\n",
        "                            h1Ii = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h0I) #inhibitory neuron inhibitory inputs\n",
        "                            h1E = tf.keras.layers.Add()([h1Ee, h1Ei])\n",
        "                            h1I = tf.keras.layers.Add()([h1Ie, h1Ii])\n",
        "                        h1E = tf.keras.layers.Activation(EIactivation)(h1E)\n",
        "                        h1I = tf.keras.layers.Activation(EIactivation)(h1I)\n",
        "                        h1 = (h1E, h1I)\n",
        "            else:\n",
        "                h1I = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0) #inhibitory interneuron (excitatory inputs)\n",
        "                if(not inhibitoryNeuronNormalisationFactorStatic):  #disabled because modifies activation levels\n",
        "                    h1I = tf.keras.layers.Activation(EIactivation)(h1I)\n",
        "                h1I = h1I*calculateInhibitoryNeuronNormalisationFactor(h0, h1I, numChannels, previousNumChannels, firstLayer)\n",
        "                h1Ee = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0) #excitatory neuron excitatory inputs\n",
        "                h1Ei = tf.keras.layers.Dense(numChannels, kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h1I) #excitatory neuron inhibitory inputs\n",
        "                h1E = tf.keras.layers.Add()([h1Ee, h1Ei])\n",
        "                h1E = tf.keras.layers.Activation(EIactivation)(h1E)\n",
        "                h1 = h1E\n",
        "    return h1\n",
        "\n",
        "def calculateAverageWeight(numChannels, previousNumChannels):\n",
        "    shape = [previousNumChannels,numChannels]\n",
        "    averageWeight = EIweightInitialisedAverage(shape)\n",
        "    #averageWeight = 1.00\n",
        "    #avg = x*sqrt(pi/2) = 1.25331413732 #https://stats.stackexchange.com/questions/363240/mean-of-absgauss-as-a-function-of-the-standard-deviation\n",
        "    return averageWeight\n",
        "\n",
        "def calculateInhibitoryNeuronNormalisationFactor(h0, h1I, numChannels, previousNumChannels, firstLayer=False):\n",
        "    if(inhibitoryNeuronNormalisationFactorStatic):\n",
        "        averageLayerActivation = 0.5    #this is not correct\n",
        "        averageWeight = calculateAverageWeight(numChannels, previousNumChannels)\n",
        "        if(firstLayer):\n",
        "            #assume input layer unequal activation/nonactivation level\n",
        "            averageLayerActivation = np.mean(x_train)\n",
        "        print(\"previousNumChannels = \", previousNumChannels)\n",
        "        print(\"averageLayerActivation = \", averageLayerActivation)\n",
        "        print(\"averageWeight = \", averageWeight)\n",
        "        h1InormalisationFactor = (1/(previousNumChannels*averageWeight*averageLayerActivation))\n",
        "    else:\n",
        "        h1InormalisationFactor = tf.reduce_mean(h0)/tf.reduce_mean(h1I)\n",
        "    return h1InormalisationFactor\n",
        "\n",
        "def concatEIneurons(h):\n",
        "    if(inhibitoryNeuronOutputPositive):\n",
        "        return h\n",
        "    else:\n",
        "        if(inlineImplementation):\n",
        "            if(positiveWeightImplementation):\n",
        "                return h\n",
        "            else:\n",
        "                if(integrateWeights):\n",
        "                    pass\n",
        "                else:\n",
        "                    hE, hI = h\n",
        "                    h = tf.keras.layers.Concatenate()([hE, hI])\n",
        "                return h\n",
        "        else:\n",
        "            return h"
      ],
      "metadata": {
        "id": "msg_fj8mwhe0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "outputs": [],
      "source": [
        "x = tf.keras.layers.Input(shape=input_shape)\n",
        "h0 = tf.keras.layers.Flatten()(x)\n",
        "hLast = h0\n",
        "\n",
        "previousNumChannels = input_shape[0]*input_shape[1]\n",
        "if(numberOfHiddenLayers >= 1):\n",
        "    numChannels = layerSizeBase*layerRatio\n",
        "    h1 = createEIlayer(1, h0, numChannels, previousNumChannels, firstLayer=True)\n",
        "    hLast = h1\n",
        "    previousNumChannels = numChannels\n",
        "if(numberOfHiddenLayers >= 2):\n",
        "    numChannels = layerSizeBase*layerRatio\n",
        "    h2 = createEIlayer(2, h1, numChannels, previousNumChannels)\n",
        "    hLast = h2\n",
        "    previousNumChannels = numChannels\n",
        "if(numberOfHiddenLayers >= 3):\n",
        "    numChannels = layerSizeBase*layerRatio\n",
        "    h3 = createEIlayer(3, h2, numChannels, previousNumChannels)\n",
        "    hLast = h3\n",
        "    previousNumChannels = numChannels\n",
        "if(numberOfHiddenLayers >= 4):\n",
        "    numChannels = layerSizeBase*layerRatio\n",
        "    h4 = createEIlayer(4, h3, numChannels, previousNumChannels)\n",
        "    hLast = h4\n",
        "    previousNumChannels = numChannels\n",
        "\n",
        "if(addSkipLayers):\n",
        "    mList = []\n",
        "    if(numberOfHiddenLayers >= 1):\n",
        "        m1 = tf.keras.layers.Flatten()(concatEIneurons(h1))\n",
        "        mList.append(m1)\n",
        "    if(numberOfHiddenLayers >= 2):\n",
        "        m2 = tf.keras.layers.Flatten()(concatEIneurons(h2))\n",
        "        mList.append(m2)\n",
        "    if(numberOfHiddenLayers >= 3):\n",
        "        m3 = tf.keras.layers.Flatten()(concatEIneurons(h3))\n",
        "        mList.append(m3)\n",
        "    if(numberOfHiddenLayers >= 4):\n",
        "        m4 = tf.keras.layers.Flatten()(concatEIneurons(h4))\n",
        "        mList.append(m4)\n",
        "    hLast = tf.keras.layers.concatenate(mList)\n",
        "else:\n",
        "    hLast = concatEIneurons(hLast)\n",
        "\n",
        "if(generateUntrainedNetwork):\n",
        "    hLast = tf.keras.layers.Lambda(lambda x: tf.keras.backend.stop_gradient(x))(hLast)\n",
        "\n",
        "if(inhibitoryNeuronOutputPositive):\n",
        "    y = tf.keras.layers.Dense(num_classes, activation='softmax')(hLast)\n",
        "else:\n",
        "    y = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_constraint=EIweightConstraintLastLayer, bias_constraint=EIbiasConstraintLastLayer)(hLast)\n",
        "model = tf.keras.Model(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RSkzdv8MD0tT"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9foNKHzTD2Vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53179251-4233-4732-85ba-5391a2b04109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28)]             0         []                            \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 784)                  0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 128)                  100480    ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  100480    ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.negative (TFOpLamb  (None, 128)                  0         ['dense_1[0][0]']             \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 128)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 128)                  0         ['tf.math.negative[0][0]']    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 256)                  0         ['activation[0][0]',          \n",
            "                                                                     'activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 128)                  32896     ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 128)                  32896     ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.negative_1 (TFOpLa  (None, 128)                  0         ['dense_3[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 128)                  0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 128)                  0         ['tf.math.negative_1[0][0]']  \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 256)                  0         ['activation_2[0][0]',        \n",
            " )                                                                   'activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 10)                   2570      ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 269322 (1.03 MB)\n",
            "Trainable params: 269322 (1.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "    #temp: model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "#printModelSummary(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(debugPreTrainWeights):\n",
        "    testwritefile = open('weightsPreTrain.txt', 'w')\n",
        "    for layerIndex, layer in enumerate(model.layers):\n",
        "        heading = \"\\n\" + \"layer = \" + str(layerIndex) + \"\\n\"\n",
        "        testwritefile.write(heading)\n",
        "        weights = layer.get_weights()\n",
        "        #weightsAvg = np.mean(weights[0])\n",
        "        #print(heading)\n",
        "        #print(\"weightsAvg = \", weightsAvg)\n",
        "        weightsS =  str(weights)\n",
        "        testwritefile.write(weightsS)\n",
        "    testwritefile.close()"
      ],
      "metadata": {
        "id": "FAQw0rvCd4Wt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(debugPreTrainOutputs):\n",
        "    testwritefile = open('outputPreTrain.txt', 'w')\n",
        "    xTrainFirstSample = np.expand_dims(x_train[0], axis=0)\n",
        "    for layerIndex, layer in enumerate(model.layers):\n",
        "        heading = \"\\n\" + \"layer = \" + str(layerIndex) + \"\\n\"\n",
        "        testwritefile.write(heading)\n",
        "        func = K.function([model.get_layer(index=0).input], layer.output)\n",
        "        layerOutput = func([xTrainFirstSample])  # input_data is a numpy array\n",
        "        #print(heading)\n",
        "        #print(\"layerOutput.shape = \", layerOutput.shape)\n",
        "        layerOutputS =  str(layerOutput)\n",
        "        testwritefile.write(layerOutputS)\n",
        "    testwritefile.close()"
      ],
      "metadata": {
        "id": "o8FNyXG9Pl-G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "5lJZhEkFYCtL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y7suUbJXVLqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd121d0-64d7-4ce9-b6ff-99c3b82e39ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 8s 8ms/step - loss: 0.2194 - accuracy: 0.9349\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0871 - accuracy: 0.9727\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0589 - accuracy: 0.9814\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0432 - accuracy: 0.9864\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0327 - accuracy: 0.9892\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x797c2891aa40>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(debugPostTrainWeights):\n",
        "    testwritefile = open('weightsPostTrain.txt', 'w')\n",
        "    for layerIndex, layer in enumerate(model.layers):\n",
        "        heading = \"\\n\" + \"layer = \" + str(layerIndex) + \"\\n\"\n",
        "        testwritefile.write(heading)\n",
        "        weights = layer.get_weights()\n",
        "        #print(heading)\n",
        "        #print(weights)\n",
        "        weightsS =  str(weights)\n",
        "        testwritefile.write(weightsS)\n",
        "    testwritefile.close()"
      ],
      "metadata": {
        "id": "nyUVIVzvNz3o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(debugPostTrainOutputs):\n",
        "    testwritefile = open('outputPostTrain.txt', 'w')\n",
        "    xTrainFirstSample = np.expand_dims(x_train[0], axis=0)\n",
        "    for layerIndex, layer in enumerate(model.layers):\n",
        "        heading = \"\\n\" + \"layer = \" + str(layerIndex) + \"\\n\"\n",
        "        testwritefile.write(heading)\n",
        "        func = K.function([model.get_layer(index=0).input], layer.output)\n",
        "        layerOutput = func([xTrainFirstSample])  # input_data is a numpy array\n",
        "        #print(heading)\n",
        "        #print(layerOutput)\n",
        "        layerOutputS = str(layerOutput)  #tf.tensor.toString(layerOutput)    #layerOutput.tostring()\n",
        "        testwritefile.write(layerOutputS)\n",
        "    testwritefile.close()"
      ],
      "metadata": {
        "id": "wkFMMGAV3DvD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model"
      ],
      "metadata": {
        "id": "yp1IpSBHYGpV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "F7dTAzgHDUh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a43a0a-6685-4905-aac4-21825dd001a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0848 - accuracy: 0.9752 - 1s/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08482825011014938, 0.9751999974250793]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rYb6DrEH0GMv"
      },
      "outputs": [],
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cnqOZtUp1YR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962c1a30-42cd-4354-eb62-d66ee88b240b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[0.0853368 , 0.0853368 , 0.08533684, 0.08533705, 0.0853368 ,\n",
              "        0.0853368 , 0.0853368 , 0.23196852, 0.0853368 , 0.08533682],\n",
              "       [0.08533674, 0.08533677, 0.23196927, 0.08533674, 0.08533674,\n",
              "        0.08533674, 0.08533674, 0.08533674, 0.08533674, 0.08533674],\n",
              "       [0.08533952, 0.23192555, 0.08534102, 0.08533955, 0.08533973,\n",
              "        0.0853397 , 0.08533981, 0.0853519 , 0.08534375, 0.08533952],\n",
              "       [0.23152587, 0.085365  , 0.08549026, 0.08536571, 0.08536474,\n",
              "        0.08542477, 0.08536538, 0.08536545, 0.08536474, 0.08536804],\n",
              "       [0.08533739, 0.08533729, 0.08533733, 0.08533729, 0.23196074,\n",
              "        0.08533729, 0.08533741, 0.08533802, 0.0853373 , 0.08533991]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "probability_model(x_test[:5])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}