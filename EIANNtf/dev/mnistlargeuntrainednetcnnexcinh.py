# -*- coding: utf-8 -*-
"""MNISTlargeUntrainedNetCNNexcInh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uts0ul0iHzAHM5nC_LcJm5eHAAsRcCjI

# MNIST Large Untrained Net CNN Exc Inh

Derived from https://www.kaggle.com/code/amyjang/tensorflow-mnist-cnn-tutorial/notebook
"""

import tensorflow as tf
import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from keras import backend as K

print(tf.__version__)

import numpy as np
import sys
np.set_printoptions(threshold=sys.maxsize)
import math

inhibitoryNeuronOutputPositive = True
if(inhibitoryNeuronOutputPositive):
    inhibitoryNeuronSwitchActivation = True
else:
    inlineImplementation = False	#orig: True #True: excitatory/inhibitory neurons are on same sublayer, False: add inhibitory neurons to separate preceding sublayer
    if(inlineImplementation):
        positiveWeightImplementation = False	#orig: True #optional
        if(not positiveWeightImplementation):
            integrateWeights = True    #orig: False #optional
            if(integrateWeights):
                integrateWeights1 = False    #explicitly declare E/I neurons
                integrateWeights2 = True    #implicitly declare E/I neurons
                integrateWeightsInitialiseZero = False   #miscellaneous training performance improvement (single EI layer only)
        excitatoryNeuronThreshold = 0.0 #mandatory
    else:
        positiveWeightImplementation = False    #False: only current coded implementation
        inhibitoryNeuronNormalisationFactorStatic = False    #True: normalise intermediary inhibitory neuron layer based on h0/h1 num neurons, False: normalise based on h0/h1 activations
        excitatoryNeuronThreshold = 0.0   #orig: 0.0

inputLayerExcitatoryOnly = True #True: only current coded implementation

generateUntrainedNetwork = False
if(generateUntrainedNetwork):
    #only train the last layer
    numberOfHiddenLayers = 2    #default: 2    #if 0 then useSVM=True
    preFinalDenseLayer = False
else:
    numberOfHiddenLayers = 2   #default: 4
    preFinalDenseLayer = False


if(numberOfHiddenLayers > 1):
    addSkipLayers = False   #optional
else:
    addSkipLayers = False   #mandatory

layerSizeBase = 32  #default: 32

batch_size = 64 #default: 64
epochs = 5  #1  #5

debugNoEIneurons = False
debugPreTrainWeights = True
debugPreTrainOutputs = True
debugPostTrainWeights = True
debugPostTrainOutputs = True
if(debugNoEIneurons):
    numberOfHiddenLayers = 4  #default = 4, if 0 then useSVM=True
    preFinalDenseLayer = True

"""## Load data"""

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

input_shape = (28, 28, 1)

x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)
x_train=x_train / 255.0
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)
x_test=x_test/255.0

y_train = tf.one_hot(y_train.astype(np.int32), depth=10)
y_test = tf.one_hot(y_test.astype(np.int32), depth=10)

print("input_shape = ", input_shape)
print("x_train.shape = ", x_train.shape)

plt.imshow(x_train[100][:,:,0])
print(y_train[100])

"""## Define model"""

num_classes = 10

def EIactivation(Z):
    A = K.maximum(Z, excitatoryNeuronThreshold)-excitatoryNeuronThreshold  #ReLU
    return A

def EIactivationExcitatory(Z):
    if(inlineImplementation):
        if(positiveWeightImplementation):
            return EIactivation(Z)
        else:
             print("EIactivationExcitatory error: requires positiveWeightImplementation")
    else:
        print("EIactivationExcitatory error: requires inlineImplementation")

def EIactivationInhibitory(Z):
    if(inlineImplementation):
        if(positiveWeightImplementation):
            return -EIactivation(Z)   #ReLU with negative output
        else:
             print("EIactivationInhibitory error: requires positiveWeightImplementation")
    else:
        print("inlineImplementation error: requires inlineImplementation")

def EIweightInitializer(shape, dtype=None):
    if(inlineImplementation):
        if(positiveWeightImplementation):
            w = tf.math.abs(tf.random.normal(shape, dtype=dtype))
        else:
            if(integrateWeights):
                if(integrateWeightsInitialiseZero):
                    w = tf.zeros(shape, dtype=dtype)    #tf.math.abs(tf.random.normal(shape, dtype=dtype))
                else:
                    #print("shape = ", shape)
                    w = tf.math.abs(tf.random.normal(shape, dtype=dtype))
                    wEIsize = w.shape[2]//2
                    wSignE = tf.ones([w.shape[0], w.shape[1], wEIsize, w.shape[3]])
                    wSignI = tf.ones([w.shape[0], w.shape[1], wEIsize, w.shape[3]])
                    wSignI = tf.multiply(wSignI, -1)
                    wSign = tf.concat([wSignE, wSignI], axis=2)
                    w = tf.multiply(w, wSign)
            else:
                print("EIweightInitializer error: requires !positiveWeightImplementation:integrateWeights")
    else:
        print("EIweightInitializer error: requires inlineImplementation")

    return w

def EIweightInitializerExcitatory(shape, dtype=None):
    if(positiveWeightImplementation):
        print("EIweightInitializerExcitatory error: requires !positiveWeightImplementation")
    else:
        return tf.math.abs(tf.random.normal(shape, dtype=dtype))

def EIweightInitializerInhibitory(shape, dtype=None):
    if(positiveWeightImplementation):
        print("EIweightInitializerExcitatory error: requires !positiveWeightImplementation")
    else:
        return tf.math.negative(tf.math.abs(tf.random.normal(shape, dtype=dtype)))

def EIweightInitialisedAverage(shape):
    return tf.reduce_mean(tf.math.abs(tf.random.normal(shape)))

class negative(tf.keras.constraints.Constraint):
    #based on https://www.tensorflow.org/api_docs/python/tf/keras/constraints/Constraint
    def __init__(self):
        pass
    def __call__(self, w):
        return w * tf.cast(tf.math.less_equal(w, 0.), w.dtype)

class positiveOrNegative(tf.keras.constraints.Constraint):
    #based on https://www.tensorflow.org/api_docs/python/tf/keras/constraints/Constraint
    def __init__(self):
        pass
    def __call__(self, w):
        w_shape = w.shape
        #print("w_shape = ", w_shape)
        wEIsize = w.shape[2]//2
        wE = w[:, :, 0:wEIsize]
        wI = w[:, :, wEIsize:]
        wEcheck = tf.greater_equal(wE, 0)
        wIcheck = tf.less_equal(wI, 0)
        wEcheck = tf.cast(wEcheck, tf.float32)
        wIcheck = tf.cast(wIcheck, tf.float32)
        wE = tf.multiply(wE, wEcheck)
        wI = tf.multiply(wI, wIcheck)
        w = tf.concat([wE, wI], axis=2)
        return w

if(not inhibitoryNeuronOutputPositive):
    if(inlineImplementation):
        if(positiveWeightImplementation):
            EIweightConstraint = tf.keras.constraints.non_neg()
            constrainBiases = True   #ensure positive biases also
            if(constrainBiases):
                EIbiasConstraint = tf.keras.constraints.non_neg()
                constrainBiasesLastLayer = False
                if(constrainBiasesLastLayer):
                    EIbiasConstraintLastLayer = tf.keras.constraints.non_neg()
                else:
                    EIbiasConstraintLastLayer = None
            else:
                EIbiasConstraint = None
                EIbiasConstraintLastLayer = None
        else:
            if(integrateWeights):
                EIweightConstraint = positiveOrNegative()
                EIbiasConstraint = None
            else:
                EIweightConstraintPositive = tf.keras.constraints.non_neg()
                EIweightConstraintNegative = negative()
                constrainBiases = False
                if(constrainBiases):
                    EIbiasConstraintPositive = tf.keras.constraints.non_neg()
                    EIbiasConstraintNegative = negative()
                else:
                    EIbiasConstraintPositive = None
                    EIbiasConstraintNegative = None
    else:
        EIweightConstraintPositive = tf.keras.constraints.non_neg()
        EIweightConstraintNegative = negative()
        constrainBiases = False
        if(constrainBiases):
            EIbiasConstraintPositive = tf.keras.constraints.non_neg()
            EIbiasConstraintNegative = negative()
        else:
            EIbiasConstraintPositive = None
            EIbiasConstraintNegative = None
        EIweightConstraintLastLayer = None
        EIbiasConstraintLastLayer = None

if(generateUntrainedNetwork):
    #only train the last layer
    generateLargeNetwork = True
else:
    generateLargeNetwork = False

if(generateLargeNetwork):
    largeNetworkRatio = 10    #100
    generateLargeNetworkExpansion = False
    if(generateLargeNetworkExpansion):
        generateLargeNetworkRatioExponential = False
else:
    generateLargeNetworkRatio = False

def getLayerRatio(layerIndex):
    layerRatio = 1
    if(generateLargeNetwork):
        if(generateLargeNetworkExpansion):
            if(generateLargeNetworkRatioExponential):
                layerRatio = largeNetworkRatio**layerIndex
            else:
                layerRatio = largeNetworkRatio * layerIndex
        else:
            layerRatio = largeNetworkRatio
    else:
        layerRatio = 1
    return int(layerRatio)

def createEIlayer(layerIndex, h0, numChannels, previousNumChannels, firstLayer=False, maxpool2d=None, dropout=None):
    layerRatio = getLayerRatio(2)
    if(debugNoEIneurons):
        h1 = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same')(h0)
        h1 = tf.keras.layers.Activation(EIactivation)(h1)
        if(maxpool2d is not None):
            h1 = tf.keras.layers.MaxPool2D(strides=maxpool2d)(h1)
        if(dropout is not None):
            h1 = tf.keras.layers.Dropout(dropout)(h1)
    else:
        if(inhibitoryNeuronOutputPositive):
            h1E = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same')(h0)
            h1I = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same')(h0)
            h1E = tf.keras.layers.Activation('relu')(h1E)
            if(inhibitoryNeuronSwitchActivation):
                h1I = tf.keras.layers.Activation('relu')(-h1I)
            else:
                h1I = tf.keras.layers.Activation('relu')(h1I)
            h1 = tf.keras.layers.Concatenate()([h1E, h1I])
            if(maxpool2d is not None):
                h1 = tf.keras.layers.MaxPool2D(strides=maxpool2d)(h1)
            if(dropout is not None):
                h1 = tf.keras.layers.Dropout(dropout)(h1)
        else:
            if(inlineImplementation):
                if(positiveWeightImplementation):
                    h1E = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)
                    h1I = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)
                    h1E = tf.keras.layers.Activation(EIactivationExcitatory)(h1E)
                    h1I = tf.keras.layers.Activation(EIactivationInhibitory)(h1I)
                    h1 = tf.keras.layers.Concatenate()([h1E, h1I])
                    if(maxpool2d is not None):
                        h1 = tf.keras.layers.MaxPool2D(strides=maxpool2d)(h1)
                    if(dropout is not None):
                        h1 = tf.keras.layers.Dropout(dropout)(h1)
                else:
                    if(integrateWeights):
                        if(integrateWeights1):
                            if(firstLayer):
                                h1E = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same')(h0)
                                h1I = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same')(h0)
                            else:
                                h1E = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)
                                h1I = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)
                            h1E = tf.keras.layers.Activation(EIactivation)(h1E)
                            h1I = tf.keras.layers.Activation(EIactivation)(h1I)
                            h1 = tf.keras.layers.Concatenate(axis=2)([h1E, h1I])
                        elif(integrateWeights2):
                            if(firstLayer):
                                h1 = tf.keras.layers.Conv2D(numChannels*2, (5,5), padding='same')(h0)
                            else:
                                h1 = tf.keras.layers.Conv2D(numChannels*2, (5,5), padding='same', kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)
                            h1 = tf.keras.layers.Activation(EIactivation)(h1)
                        if(maxpool2d is not None):
                            h1 = tf.keras.layers.MaxPool2D(strides=maxpool2d)(h1)
                        if(dropout is not None):
                            h1 = tf.keras.layers.Dropout(dropout)(h1)
                    else:
                        if(firstLayer):
                            h1E = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same')(h0)
                            h1I = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same')(h0)
                        else:
                            h0E, h0I = h0
                            h1Ee = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0E) #excitatory neuron excitatory inputs
                            h1Ei = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h0I) #excitatory neuron inhibitory inputs
                            h1Ie = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0E) #inhibitory neuron excitatory inputs
                            h1Ii = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h0I) #inhibitory neuron inhibitory inputs
                            h1E = tf.keras.layers.Add()([h1Ee, h1Ei])
                            h1I = tf.keras.layers.Add()([h1Ie, h1Ii])
                        h1E = tf.keras.layers.Activation(EIactivation)(h1E)
                        h1I = tf.keras.layers.Activation(EIactivation)(h1I)
                        if(maxpool2d is not None):
                            h1E = tf.keras.layers.MaxPool2D(strides=maxpool2d)(h1E)
                            h1I = tf.keras.layers.MaxPool2D(strides=maxpool2d)(h1I)
                        if(dropout is not None):
                            h1E = tf.keras.layers.Dropout(dropout)(h1E)
                            h1I = tf.keras.layers.Dropout(dropout)(h1I)
                        h1 = (h1E, h1I)
            else:
                if(firstLayer):
                    previousNumChannels = 1
                else:
                    previousNumChannels = layerSizeBase*layerRatio
                h1I = tf.keras.layers.Conv2D(previousNumChannels, (5,5), padding='same', kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0) #inhibitory interneuron (excitatory inputs)
                if(not inhibitoryNeuronNormalisationFactorStatic):  #disabled because modifies activation levels
                    h1I = tf.keras.layers.Activation(EIactivation)(h1I)
                h1I = h1I*calculateInhibitoryNeuronNormalisationFactor(h0, h1I, numChannels, previousNumChannels, firstLayer)
                h1Ee = tf.keras.layers.Conv2D(layerSizeBase*layerRatio, (5,5), padding='same', kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0) #excitatory neuron excitatory inputs
                h1Ei = tf.keras.layers.Conv2D(layerSizeBase*layerRatio, (5,5), padding='same', kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h1I) #excitatory neuron inhibitory inputs
                h1E = tf.keras.layers.Add()([h1Ee, h1Ei])
                h1E = tf.keras.layers.Activation(EIactivation)(h1E)
                h1 = h1E
                if(maxpool2d is not None):
                    h1 = tf.keras.layers.MaxPool2D(strides=maxpool2d)(h1)
                if(dropout is not None):
                    h1 = tf.keras.layers.Dropout(dropout)(h1)
    return h1

def calculateAverageWeight(numChannels, previousNumChannels):
    shape = [previousNumChannels,numChannels]
    averageWeight = EIweightInitialisedAverage(shape)
    #avg = x*sqrt(pi/2) = 1.25331413732 #https://stats.stackexchange.com/questions/363240/mean-of-absgauss-as-a-function-of-the-standard-deviation
    return averageWeight

def calculateInhibitoryNeuronNormalisationFactor(h0, h1I, numChannels, previousNumChannels, firstLayer=False):
    if(inhibitoryNeuronNormalisationFactorStatic):
        previousNumChannels = previousNumChannels*input_shape[1]*input_shape[2]
        numChannels = numChannels*input_shape[1]*input_shape[2]
        averageLayerActivation = 0.5    #this is not correct
        averageWeight = calculateAverageWeight(numChannels, previousNumChannels)
        if(firstLayer):
            #assume input layer unequal activation/nonactivation level
            averageLayerActivation = np.mean(x_train)
        print("previousNumChannels = ", previousNumChannels)
        print("averageLayerActivation = ", averageLayerActivation)
        print("averageWeight = ", averageWeight)
        h1InormalisationFactor = (1/previousNumChannels*averageWeight)*averageLayerActivation
    else:
        h1InormalisationFactor = tf.reduce_mean(h0)/tf.reduce_mean(h1I)
    return h1InormalisationFactor

def concatEIneurons(h):
    if(inhibitoryNeuronOutputPositive):
        return h
    else:
        if(inlineImplementation):
            if(positiveWeightImplementation):
                return h
            else:
                if(integrateWeights):
                    pass
                else:
                    hE, hI = h
                    h = tf.keras.layers.Concatenate()([hE, hI])
                return h
        else:
            return h

x = tf.keras.layers.Input(shape=input_shape)
h0 = x
hLast = h0

previousNumChannels = input_shape[2]    #1
if(numberOfHiddenLayers >= 1):
    numChannels = layerSizeBase*getLayerRatio(1)*1
    h1 = createEIlayer(1, h0, numChannels, previousNumChannels, firstLayer=True)
    hLast = h1
if(numberOfHiddenLayers >= 2):
    numChannels = layerSizeBase*getLayerRatio(2)*1
    h2 = createEIlayer(2, h1, numChannels, previousNumChannels, maxpool2d=(1,1), dropout=0.25)
    hLast = h2
if(numberOfHiddenLayers >= 3):
    numChannels = layerSizeBase*getLayerRatio(3)*2
    h3 = createEIlayer(3, h2, numChannels, previousNumChannels)
    hLast = h3
if(numberOfHiddenLayers >= 4):
    numChannels = layerSizeBase*getLayerRatio(4)*2
    h4 = createEIlayer(4, h3, numChannels, previousNumChannels, maxpool2d=(2,2))
    hLast = h4

if(addSkipLayers):
    mList = []
    if(numberOfHiddenLayers >= 1):
        m1 = tf.keras.layers.Flatten()(concatEIneurons(h1))
        mList.append(m1)
    if(numberOfHiddenLayers >= 2):
        m2 = tf.keras.layers.Flatten()(concatEIneurons(h2))
        mList.append(m2)
    if(numberOfHiddenLayers >= 3):
        m3 = tf.keras.layers.Flatten()(concatEIneurons(h3))
        mList.append(m3)
    if(numberOfHiddenLayers >= 4):
        m4 = tf.keras.layers.Flatten()(concatEIneurons(h4))
        mList.append(m4)
    hLast = tf.keras.layers.concatenate(mList)
else:
    hLast = concatEIneurons(hLast)

hLast = tf.keras.layers.Flatten()(hLast)
if(preFinalDenseLayer):
    hLast = tf.keras.layers.Dense(128, activation='relu')(hLast)
    hLast = tf.keras.layers.Dropout(0.5)(hLast)

if(generateUntrainedNetwork):
    hLast = tf.keras.layers.Lambda(lambda x: tf.keras.backend.stop_gradient(x))(hLast)

y = tf.keras.layers.Dense(num_classes, activation='softmax')(hLast)
model = tf.keras.Model(x, y)

print(model.summary())
#printModelSummary(model)

model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('acc')>0.995):
      print("\nReached 99.5% accuracy so cancelling training!")
      self.model.stop_training = True

callbacks = myCallback()

if(debugPreTrainWeights):
    testwritefile = open('weightsPreTrain.txt', 'w')
    for layerIndex, layer in enumerate(model.layers):
        heading = "\n" + "layer = " + str(layerIndex) + "\n"
        testwritefile.write(heading)
        weights = layer.get_weights()
        #weightsAvg = np.mean(weights[0])
        #print(heading)
        #print(weights)
        weightsS =  str(weights)
        testwritefile.write(weightsS)
    testwritefile.close()

if(debugPreTrainOutputs):
    testwritefile = open('outputPreTrain.txt', 'w')
    xTrainFirstSample = np.expand_dims(x_train[0], axis=0)
    for layerIndex, layer in enumerate(model.layers):
        heading = "\n" + "layer = " + str(layerIndex) + "\n"
        testwritefile.write(heading)
        func = K.function([model.get_layer(index=0).input], layer.output)
        layerOutput = func([xTrainFirstSample])  # input_data is a numpy array
        #print(heading)
        #print(layerOutput)
        layerOutputS =  str(layerOutput)
        testwritefile.write(layerOutputS)
    testwritefile.close()

"""## Train model"""

history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_split=0.1,
                    callbacks=[callbacks])

if(debugPostTrainWeights):
    testwritefile = open('weightsPostTrain.txt', 'w')
    for layerIndex, layer in enumerate(model.layers):
        heading = "\n" + "layer = " + str(layerIndex) + "\n"
        testwritefile.write(heading)
        weights = layer.get_weights()
        #print(heading)
        #print(weights)
        weightsS =  str(weights)
        testwritefile.write(weightsS)
    testwritefile.close()

if(debugPostTrainOutputs):
    testwritefile = open('outputPostTrain.txt', 'w')
    xTrainFirstSample = np.expand_dims(x_train[0], axis=0)
    for layerIndex, layer in enumerate(model.layers):
        heading = "\n" + "layer = " + str(layerIndex) + "\n"
        testwritefile.write(heading)
        func = K.function([model.get_layer(index=0).input], layer.output)
        layerOutput = func([xTrainFirstSample])  # input_data is a numpy array
        #print(heading)
        #print(layerOutput)
        layerOutputS =  str(layerOutput)
        testwritefile.write(layerOutputS)
    testwritefile.close()

"""## Evaluate model"""

fig, ax = plt.subplots(2,1)
ax[0].plot(history.history['loss'], color='b', label="Training Loss")
ax[0].plot(history.history['val_loss'], color='r', label="Validation Loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True)

ax[1].plot(history.history['acc'], color='b', label="Training Accuracy")
ax[1].plot(history.history['val_acc'], color='r',label="Validation Accuracy")
legend = ax[1].legend(loc='best', shadow=True)

test_loss, test_acc = model.evaluate(x_test, y_test)